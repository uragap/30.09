{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 Les auteurs de TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Différenciation automatique avancée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/advanced_autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Voir sur TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Exécuter dans Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Afficher la source sur GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Télécharger le carnet</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "Le [guide de différenciation automatique](autodiff.ipynb) comprend tout ce qui est nécessaire pour calculer les dégradés. Ce guide se concentre sur les fonctionnalités plus profondes et moins courantes de l' `tf.GradientTape` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Installer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uGRJJRi8TCkJ"
      },
      "source": [
        "## Contrôle de l'enregistrement en dégradé\n",
        "\n",
        "Dans le [guide de différenciation automatique,](autodiff.ipynb) vous avez vu comment contrôler les variables et les tenseurs surveillés par la bande lors de la construction du calcul de gradient.\n",
        "\n",
        "La bande a également des méthodes pour manipuler l'enregistrement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gB_i0VnhQKt2"
      },
      "source": [
        "Si vous souhaitez arrêter l'enregistrement des dégradés, vous pouvez utiliser `GradientTape.stop_recording()` pour suspendre temporairement l'enregistrement.\n",
        "\n",
        "Cela peut être utile pour réduire les frais généraux si vous ne souhaitez pas différencier une opération compliquée au milieu de votre modèle. Cela peut inclure le calcul d'une métrique ou d'un résultat intermédiaire:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "mhFSYf7uQWxR"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  x_sq = x * x\n",
        "  with t.stop_recording():\n",
        "    y_sq = y * y\n",
        "  z = x_sq + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DEHbEZ1h4p8A"
      },
      "source": [
        "Si vous souhaitez tout recommencer, utilisez `reset()` . Il est généralement plus facile de lire simplement la sortie du bloc de bande dégradé et le redémarrage, mais vous pouvez utiliser la `reset` lorsque la sortie du bloc de bande est difficile, voire impossible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "lsMHsmrh4pqM"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "reset = True\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y * y\n",
        "  if reset:\n",
        "    # Throw out all the tape recorded so far\n",
        "    t.reset()\n",
        "  z = x * x + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6zS7cLmS6zMf"
      },
      "source": [
        "## Arrêter le dégradé\n",
        "\n",
        "Contrairement aux contrôles de bande globaux ci-dessus, la fonction `tf.stop_gradient` est beaucoup plus précise. Il peut être utilisé pour empêcher les dégradés de s'écouler le long d'un chemin particulier, sans avoir besoin d'accéder à la bande elle-même:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "30qnZMe48BkB"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y**2\n",
        "  z = x**2 + tf.stop_gradient(y_sq)\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mbb-9lnGVngH"
      },
      "source": [
        "## Dégradés personnalisés\n",
        "\n",
        "Dans certains cas, vous souhaiterez peut-être contrôler exactement la façon dont les dégradés sont calculés plutôt que d'utiliser la valeur par défaut. Ces situations comprennent:\n",
        "\n",
        "- Il n'y a pas de dégradé défini pour une nouvelle opération que vous écrivez.\n",
        "- Les calculs par défaut sont numériquement instables.\n",
        "- Vous souhaitez mettre en cache un calcul coûteux de la passe avant.\n",
        "- Vous souhaitez modifier une valeur (par exemple en utilisant: `tf.clip_by_value` , `tf.math.round` ) sans modifier le dégradé.\n",
        "\n",
        "Pour écrire un nouvel op, vous pouvez utiliser `tf.RegisterGradient` pour configurer le vôtre. Consultez cette page pour plus de détails. (Notez que le registre de dégradés est global, changez-le avec prudence.)\n",
        "\n",
        "Pour les trois derniers cas, vous pouvez utiliser `tf.custom_gradient` .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oHr31kc_irF_"
      },
      "source": [
        "Voici un exemple qui applique `tf.clip_by_norm` au gradient intermédiaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Mjj01w4NYtwd"
      },
      "outputs": [

      ],
      "source": [
        "# Establish an identity operation, but clip during the gradient pass\n",
        "@tf.custom_gradient\n",
        "def clip_gradients(y):\n",
        "  def backward(dy):\n",
        "    return tf.clip_by_norm(dy, 0.5)\n",
        "  return y, backward\n",
        "\n",
        "v = tf.Variable(2.0)\n",
        "with tf.GradientTape() as t:\n",
        "  output = clip_gradients(v * v)\n",
        "print(t.gradient(output, v))  # calls \"backward\", which clips 4 to 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n4t7S0scYrD3"
      },
      "source": [
        "Voir le décorateur `tf.custom_gradient` pour plus de détails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8aENEt6Veryb"
      },
      "source": [
        "## Plusieurs bandes\n",
        "\n",
        "Plusieurs bandes interagissent de manière transparente. Par exemple, ici chaque bande regarde un ensemble différent de tenseurs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "BJ0HdMvte0VZ"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.constant(0.0)\n",
        "x1 = tf.constant(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
        "  tape0.watch(x0)\n",
        "  tape1.watch(x1)\n",
        "\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.sigmoid(x1)\n",
        "\n",
        "  y = y0 + y1\n",
        "\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "6ApAoMNFfNz6"
      },
      "outputs": [

      ],
      "source": [
        "tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "rF1jrAJsfYW_"
      },
      "outputs": [

      ],
      "source": [
        "tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DK05KXrAAld3"
      },
      "source": [
        "### Dégradés d'ordre supérieur\n",
        "\n",
        "Les opérations à l'intérieur du gestionnaire de contexte `GradientTape` sont enregistrées pour une différenciation automatique. Si les gradients sont calculés dans ce contexte, le calcul du gradient est également enregistré. En conséquence, la même API fonctionne également pour les dégradés d'ordre supérieur. Par exemple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "cPQgthZ7ugRJ"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    y = x * x * x\n",
        "\n",
        "  # Compute the gradient inside the outer `t2` context manager\n",
        "  # which means the gradient computation is differentiable as well.\n",
        "  dy_dx = t1.gradient(y, x)\n",
        "d2y_dx2 = t2.gradient(dy_dx, x)\n",
        "\n",
        "print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0\n",
        "print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0HV-Ah4_76i"
      },
      "source": [
        "Bien que cela vous donne la deuxième dérivée d'une fonction *scalaire* , ce modèle ne se généralise pas pour produire une matrice de Hesse, puisque `GradientTape.gradient` ne calcule que le gradient d'un scalaire. Pour construire un Hessian, voir l' [exemple de Hessian](#hessian) sous la [section Jacobian](#jacobians) .\n",
        "\n",
        "\"Appels imbriqués à `GradientTape.gradient` \" est un bon modèle lorsque vous calculez un scalaire à partir d'un dégradé, puis le scalaire résultant agit comme une source pour un deuxième calcul de gradient, comme dans l'exemple suivant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t7LRlcpVKHv1"
      },
      "source": [
        "#### Exemple: régularisation du gradient d'entrée\n",
        "\n",
        "De nombreux modèles sont susceptibles de donner des «exemples contradictoires». Cet ensemble de techniques modifie l'entrée du modèle pour confondre la sortie du modèle. La [mise en œuvre la plus simple](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) prend une seule étape le long du gradient de la sortie par rapport à l'entrée; le \"gradient d'entrée\".\n",
        "\n",
        "Une technique pour accroître la robustesse aux exemples contradictoires est [la régularisation du gradient d'entrée](https://arxiv.org/abs/1905.11468) , qui tente de minimiser l'ampleur du gradient d'entrée. Si le gradient d'entrée est petit, alors le changement dans la sortie doit également être petit.\n",
        "\n",
        "Voici une implémentation naïve de la régularisation du gradient d'entrée. La mise en œuvre est:\n",
        "\n",
        "1. Calculez le gradient de la sortie par rapport à l'entrée à l'aide d'une bande intérieure.\n",
        "2. Calculez la magnitude de ce gradient d'entrée.\n",
        "3. Calculez le gradient de cette grandeur par rapport au modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tH3ZFuUfDLrR"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "E6yOFsjEDR9u"
      },
      "outputs": [

      ],
      "source": [
        "with tf.GradientTape() as t2:\n",
        "  # The inner tape only takes the gradient with respect to the input,\n",
        "  # not the variables.\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
        "    t1.watch(x)\n",
        "    y = layer(x)\n",
        "    out = tf.reduce_sum(layer(x)**2)\n",
        "  # 1. Calculate the input gradient.\n",
        "  g1 = t1.gradient(out, x)\n",
        "  # 2. Calculate the magnitude of the input gradient.\n",
        "  g1_mag = tf.norm(g1)\n",
        "\n",
        "# 3. Calculate the gradient of the magnitude with respect to the model.\n",
        "dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "123QMq6PqK_d"
      },
      "outputs": [

      ],
      "source": [
        "[var.shape for var in dg1_mag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E4xiYigexMtQ"
      },
      "source": [
        "## Jacobiens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4-hVHVIeExkI"
      },
      "source": [
        "Tous les exemples précédents ont pris les gradients d'une cible scalaire par rapport à certains tenseur (s) source.\n",
        "\n",
        "La [matrice jacobienne](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) représente les gradients d'une fonction à valeur vectorielle. Chaque ligne contient le dégradé de l'un des éléments du vecteur.\n",
        "\n",
        "La méthode `GradientTape.jacobian` vous permet de calculer efficacement une matrice jacobienne."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KzNyIM0QBYIH"
      },
      "source": [
        "Notez que:\n",
        "\n",
        "- Comme `gradient` : l'argument `sources` peut être un tenseur ou un conteneur de tenseurs.\n",
        "- Contrairement au `gradient` : le tenseur `target` doit être un seul tenseur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O74K3hlxBC8a"
      },
      "source": [
        "### Source scalaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B08OKn1Orkuc"
      },
      "source": [
        "Comme premier exemple, voici le jacobien d'une cible vectorielle par rapport à une source scalaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "bAFeIE8EuVIq"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "delta = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = tf.nn.sigmoid(x+delta)\n",
        "\n",
        "dy_dx = tape.jacobian(y, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BgHbUk3zr-WU"
      },
      "source": [
        "Lorsque vous prenez le jacobien par rapport à un scalaire, le résultat a la forme de la **cible** et donne le gradient de chaque élément par rapport à la source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "iZ6awnDzr_BA"
      },
      "outputs": [

      ],
      "source": [
        "print(y.shape)\n",
        "print(dy_dx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "siNZaklc0_-e"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x.numpy(), y, label='y')\n",
        "plt.plot(x.numpy(), dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DsOMSD_1BGkD"
      },
      "source": [
        "### Source de tenseur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g3iXKN7KF-st"
      },
      "source": [
        "Que l'entrée soit scalaire ou tenseur, `GradientTape.jacobian` calcule efficacement le gradient de chaque élément de la source par rapport à chaque élément de la ou des cibles.\n",
        "\n",
        "Par exemple, la sortie de ce calque a la forme `(10, 7)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "39YXItgLxMBk"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = layer(x)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tshNRtfKuVP_"
      },
      "source": [
        "Et la forme du noyau du calque est `(5, 10)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CigTWyfPvPuv"
      },
      "outputs": [

      ],
      "source": [
        "layer.kernel.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mN96JRpnAjpx"
      },
      "source": [
        "La forme du jacobien de la sortie par rapport au noyau est ces deux formes concaténées ensemble:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "pRLzTTbvEimH"
      },
      "outputs": [

      ],
      "source": [
        "j = tape.jacobian(y, layer.kernel)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Lrv7miMvTll"
      },
      "source": [
        "Si vous additionnez les dimensions de la cible, vous vous retrouvez avec le gradient de la somme qui aurait été calculée par `GradientTape.gradient` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "FJjZpYRnDjVa"
      },
      "outputs": [

      ],
      "source": [
        "g = tape.gradient(y, layer.kernel)\n",
        "print('g.shape:', g.shape)\n",
        "\n",
        "j_sum = tf.reduce_sum(j, axis=[0, 1])\n",
        "delta = tf.reduce_max(abs(g - j_sum)).numpy()\n",
        "assert delta < 1e-3\n",
        "print('delta:', delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZKajuGlk_krs"
      },
      "source": [
        "<a id=\"hessian\"> </a>\n",
        "\n",
        "#### Exemple: Hessian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NYcsXeo8TDLi"
      },
      "source": [
        "Bien que `tf.GradientTape` ne donne pas de méthode explicite pour construire une matrice de Hesse, il est possible d'en construire une en utilisant la méthode `GradientTape.jacobian` .\n",
        "\n",
        "Remarque: la matrice de Hesse contient `N**2` paramètres. Pour cette raison et d'autres, ce n'est pas pratique pour la plupart des modèles. Cet exemple est davantage présenté comme une démonstration de l'utilisation de la méthode `GradientTape.jacobian` , et ne constitue pas une approbation de l'optimisation directe basée sur la Hesse. Un produit vectoriel de Hesse peut être [calculé efficacement avec des bandes imbriquées](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/hvp_test.py) et constitue une approche beaucoup plus efficace de l'optimisation du second ordre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ELGTaell_j81"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    x = layer1(x)\n",
        "    x = layer2(x)\n",
        "    loss = tf.reduce_mean(x**2)\n",
        "\n",
        "  g = t1.gradient(loss, layer1.kernel)\n",
        "\n",
        "h = t2.jacobian(g, layer1.kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "FVqQuZj4XGjm"
      },
      "outputs": [

      ],
      "source": [
        "print(f'layer.kernel.shape: {layer1.kernel.shape}')\n",
        "print(f'h.shape: {h.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_M7XElgaiMeP"
      },
      "source": [
        "Pour utiliser ce Hessian pour une étape de méthode de Newton, vous devez d'abord aplatir ses axes dans une matrice et aplatir le dégradé en un vecteur:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "6te7N6wVXwXX"
      },
      "outputs": [

      ],
      "source": [
        "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
        "\n",
        "g_vec = tf.reshape(g, [n_params, 1])\n",
        "h_mat = tf.reshape(h, [n_params, n_params])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L9rO8b-0mgOH"
      },
      "source": [
        "La matrice de Hesse doit être symétrique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8TCHc7Vrf52S"
      },
      "outputs": [

      ],
      "source": [
        "def imshow_zero_center(image, **kwargs):\n",
        "  lim = tf.reduce_max(abs(image))\n",
        "  plt.imshow(image, vmin=-lim, vmax=lim, cmap='seismic', **kwargs)\n",
        "  plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DExOxd7Ok2H0"
      },
      "outputs": [

      ],
      "source": [
        "imshow_zero_center(h_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13fBswmtQes4"
      },
      "source": [
        "L'étape de mise à jour de la méthode de Newton est illustrée ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "3DdnbynBdSor"
      },
      "outputs": [

      ],
      "source": [
        "eps = 1e-3\n",
        "eye_eps = tf.eye(h_mat.shape[0])*eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zPdtyoWeUeV"
      },
      "source": [
        "Remarque: [n'inversez pas réellement la matrice](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "k1LYftgmswOO"
      },
      "outputs": [

      ],
      "source": [
        "# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
        "# h_mat = ∇²f(X(k))\n",
        "# g_vec = ∇f(X(k))\n",
        "update = tf.linalg.solve(h_mat + eye_eps, g_vec)\n",
        "\n",
        "# Reshape the update and apply it to the variable.\n",
        "_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pF6qjlHKWxF4"
      },
      "source": [
        "Bien que cela soit relativement simple pour une seule `tf.Variable` , l'appliquer à un modèle non trivial nécessiterait une concaténation et un découpage minutieux pour produire un Hessian complet sur plusieurs variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PQWM0uN-GO5t"
      },
      "source": [
        "### Lot Jacobien"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKtB3rY6EySJ"
      },
      "source": [
        "Dans certains cas, vous voulez prendre le Jacobien de chacune d'une pile de cibles par rapport à une pile de sources, où les Jacobiens pour chaque paire cible-source sont indépendants.\n",
        "\n",
        "Par exemple, ici l'entrée `x` est mise en forme `(batch, ins)` et la sortie `y` est mise en forme `(batch, outs)` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tQMndhIUHMes"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = layer2(y)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ff2spRHEJXBU"
      },
      "source": [
        "Le jacobien complet de `y` par rapport à `x` a la forme `(batch, ins, batch, outs)` , même si vous ne voulez que `(batch, ins, outs)` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "1zSl2A5-HhMH"
      },
      "outputs": [

      ],
      "source": [
        "j = tape.jacobian(y, x)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UibJijPLJrpQ"
      },
      "source": [
        "Si les gradients de chaque élément de la pile sont indépendants, alors chaque tranche `(batch, batch)` de ce tenseur est une matrice diagonale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ZFl9uj3ueVSH"
      },
      "outputs": [

      ],
      "source": [
        "imshow_zero_center(j[:, 0, :, 0])\n",
        "_ = plt.title('A (batch, batch) slice')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "g4ZoRJcJNmy5"
      },
      "outputs": [

      ],
      "source": [
        "def plot_as_patches(j):\n",
        "  # Reorder axes so the diagonals will each form a contiguous patch.\n",
        "  j = tf.transpose(j, [1, 0, 3, 2])\n",
        "  # Pad in between each patch.\n",
        "  lim = tf.reduce_max(abs(j))\n",
        "  j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],\n",
        "             constant_values=-lim)\n",
        "  # Reshape to form a single image.\n",
        "  s = j.shape\n",
        "  j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])\n",
        "  imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])\n",
        "\n",
        "plot_as_patches(j)\n",
        "_ = plt.title('All (batch, batch) slices are diagonal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OXpTBKyeK84z"
      },
      "source": [
        "Pour obtenir le résultat souhaité, vous pouvez additionner la dimension du `batch` double, ou bien sélectionner les diagonales à l'aide de `tf.einsum` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "v65OAjEgLQwl"
      },
      "outputs": [

      ],
      "source": [
        "j_sum = tf.reduce_sum(j, axis=2)\n",
        "print(j_sum.shape)\n",
        "j_select = tf.einsum('bxby->bxy', j)\n",
        "print(j_select.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zT_VfR6lcwxD"
      },
      "source": [
        "Il serait beaucoup plus efficace de faire le calcul sans la dimension supplémentaire en premier lieu. C'est exactement ce que fait la méthode `GradientTape.batch_jacobian` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "YJLIl9WpHqYq"
      },
      "outputs": [

      ],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "jb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "-5t_q5SfHw7T"
      },
      "outputs": [

      ],
      "source": [
        "error = tf.reduce_max(abs(jb - j_sum))\n",
        "assert error < 1e-3\n",
        "print(error.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUeY2ZCiL31I"
      },
      "source": [
        "Attention: `GradientTape.batch_jacobian` vérifie uniquement que la première dimension de la source et de la cible correspondent. Il ne vérifie pas que les dégradés sont réellement indépendants. C'est à l'utilisateur de s'assurer qu'il n'utilise `batch_jacobian` là où cela a du sens. Par exemple, l'ajout d'un `layers.BatchNormalization` détruit l'indépendance, car il se normalise dans la dimension du `batch` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tnDugVc-L4fj"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "bn = tf.keras.layers.BatchNormalization()\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = bn(y, training=True)\n",
        "  y = layer2(y)\n",
        "\n",
        "j = tape.jacobian(y, x)\n",
        "print(f'j.shape: {j.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "SNyZ1WhJMVLm"
      },
      "outputs": [

      ],
      "source": [
        "plot_as_patches(j)\n",
        "\n",
        "_ = plt.title('These slices are not diagonal')\n",
        "_ = plt.xlabel(\"Don't use `batch_jacobian`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M_x7ih5sarvG"
      },
      "source": [
        "Dans ce cas, `batch_jacobian` s'exécute toujours et retourne *quelque chose* avec la forme attendue, mais son contenu n'a pas une signification claire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "k8_mICHoasCi"
      },
      "outputs": [

      ],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "print(f'jb.shape: {jb.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "advanced_autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
